{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76226e53-fb40-4568-938b-b2cdc6c24f65",
   "metadata": {},
   "source": [
    "## This notebook is for solving question 4 assignment 1 of the course \"Optimization Methods\" by Prof. Eran Treister in Ben Gurion University of the Negev\n",
    "#### This notebook include not only the raw computations but the explanations and proofs as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcfa6425-b25e-463b-9986-0176ed80154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this will limit floating numbers to 4 numbers after the point \n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed1477b-18f5-46ac-9633-68785d127c18",
   "metadata": {},
   "source": [
    "### Section (a)\n",
    "\n",
    "#### Definitions\n",
    "We are given a matrix $A$ and a vector $b$, both are specific, numerically.\n",
    "We need to write the normal equations and solve the problem using a computer. We can use built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1b3a9a-960b-4f9f-af93-77ff656f02aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# define the given matrix A - shape is 4x3\n",
    "A = np.array([[2, 1, 2], \n",
    "              [1, -2, 1], \n",
    "              [1, 2, 3], \n",
    "              [1, 1, 1]], dtype=float)\n",
    "\n",
    "print(A.shape)\n",
    "\n",
    "# define the given vector b - shape 4x1\n",
    "b = np.array([6, 1, 5, 2])  # 1D array - shape = (4,)\n",
    "b = b.reshape(-1, 1)  # 2D column vector - shape = (4, 1); the '-1' is for automatically figure out the dimension\n",
    "\n",
    "print (b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c34980a-322f-4f3d-917c-5a530b05325b",
   "metadata": {},
   "source": [
    "#### Normal equations\n",
    "The normal equations are defined as: \n",
    "\n",
    "$$\n",
    "A^T A x = A^T b\n",
    "$$\n",
    "\n",
    "We can't assume, and we will not check whether $A^TA$ is invertible.\n",
    "\n",
    "Thus, we are going to use numpy linalg.lstsq() function, which, as the name suggest, refers to least squares.\n",
    "\n",
    "np.linalg.lstsq() will provide a solution, whether $A^T A$ is invertable or not; as we already proved in question 3, there's always at least one solution.\n",
    "If there's not only one unique solution, np.linalg.lstsq() will return the solution x that has the smallest l2 norm - $||x||_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1978a0b-8302-4007-9790-9c667619c258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS Solution:\n",
      "[[1.7]\n",
      " [0.6]\n",
      " [0.7]]\n"
     ]
    }
   ],
   "source": [
    "# linalg.lstsq will return the least squares solution (one of many, possibly), the residuals (one residual in our case - b is only one column),\n",
    "# rank of A, and something that related to the SVD form of A\n",
    "x_star, residual, A_rank, _ = np.linalg.lstsq(A, b, rcond=None)  # rcond related to the SVD form of A which we haven't learned yet, so we won't talk about it here\n",
    "\n",
    "print(f\"LS Solution:\\n{x_star}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349a3a3-d6e2-45cd-af90-264343868d12",
   "metadata": {},
   "source": [
    "### Section (b)\n",
    "\n",
    "We can check whether our x_star is unique by figuring whether $A$ is full rank or not: in our case A is full rank if $rank(A)$ = 3.\n",
    "\n",
    "If $A$ is full rank, then the solution is unique.\n",
    "\n",
    "The reason is derived from the course notes, notes (3) page 31: if $A$ is full rank, then $A^TA$ is invertible, thus we have one normal equation, which implies one solution.\n",
    "\n",
    "Note: we could also prove this by the properties of $A^TA$ and the info about $A$ full rank - we would need to talk about positive semi-definite and positive definite matrices.\n",
    "But we assume it's ok to use the class notes.\n",
    "\n",
    "Remember, we already have $A$ rank from np.linalg.lstsq() in the previous section, we can just print it and see if it's 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a4432ad-6c51-4231-9f15-1a78924a534e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(A_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493be308-87e6-4587-8e2f-8b720c895217",
   "metadata": {},
   "source": [
    "As we can see, $rank(A)$ is indeed 3, which means $A$ is full rank, thus, x_star is the unique solution.\n",
    "\n",
    "Now we need to find the minimal objective (loss) value of \n",
    "$$\n",
    "||Ax^* - b||_2^2.\n",
    "$$\n",
    "This is exactly the squared 2-norm residual: \n",
    "$$\n",
    "||r(x^*)||_2^2\n",
    "$$\n",
    "We have this value from our calculation using np.linalg.lstsq()\n",
    "\n",
    "Let's print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1337c73d-6c82-40aa-8dbd-346b305daf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared residual norm (loss): 1.4\n"
     ]
    }
   ],
   "source": [
    "loss = residual[0]  # residual is of shape (1,)\n",
    "print(f\"Squared residual norm (loss): {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c51078-ee8c-422e-bab1-45a430b08c23",
   "metadata": {},
   "source": [
    "### Section (c)\n",
    "\n",
    "Here we need to calculate the *vector* \n",
    "$$\n",
    "r = Ax^* - b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd378999-f707-48f5-98b2-2c9e1fd9128f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6]\n",
      " [ 0.2]\n",
      " [ 0. ]\n",
      " [ 1. ]]\n"
     ]
    }
   ],
   "source": [
    "r = A @ x_star - b\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04897b75-bb18-409e-87bf-90898fbfae7d",
   "metadata": {},
   "source": [
    "Let's check if $A^Tr = 0$ as requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32c9b22f-d4f5-4480-b0b5-7b0e8f974bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.33066907e-15]\n",
      " [-3.10862447e-15]\n",
      " [-3.33066907e-15]]\n"
     ]
    }
   ],
   "source": [
    "A_trsp_r = A.T @ r\n",
    "print (A_trsp_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0ee8b-0187-46b7-9066-76d2333de9ae",
   "metadata": {},
   "source": [
    "This is what is called, numerically zero, it's happens due floating point stuff which discussed in class.\n",
    "We can get verify that A^Tr is numerically close to 0 with np.allclose if we want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8013f6e9-7156-4a29-bd14-7f99cc534750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(A_trsp_r, np.zeros_like(A_trsp_r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06b2df-8e71-4ff2-b33f-03227f3153de",
   "metadata": {},
   "source": [
    "Is $A^T r = 0$ suprising?\n",
    "\n",
    "No.\n",
    "\n",
    "The solution $x^*$ satisfies the normal equations:\n",
    "$$\n",
    "A^⊤Ax^∗ = A^⊤b\n",
    "$$\n",
    "\n",
    "Rewriting:\n",
    "$$\n",
    "A^⊤(Ax^∗−b) = 0 ⇒ A^⊤r = 0\n",
    "$$\n",
    "\n",
    "It's actually have a more \"wordy\" sense, $r$, the residual vector, is always orthogonal to the column space of $A$, that's the core idea of LS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5feaf46-7f03-4806-89bf-4b7bc26eda91",
   "metadata": {},
   "source": [
    "### Section (d)\n",
    "\n",
    "Here we are basically asked that, in the vector $r$, the second row (one element), will be smaller - smaller error\n",
    "\n",
    "We can do it by putting extra importance (weight) on the second row of the system. \n",
    "\n",
    "As requested, $A$ and $b$ stay the same as in section (a).\n",
    "\n",
    "So now we need to solve \n",
    "$$\n",
    "argmin_x∥Ax−b∥_W^2 \n",
    "$$\n",
    "Where $W$ is a diagonal matrix with $MxM$ shape. $W$ is also positive definite matrix. Only the second row of $W$ will be altered (others will be 1 to not affect the other equations).\n",
    "\n",
    "The normal equation here is: \n",
    "$$\n",
    "A^TWAx = A^TWb\n",
    "$$\n",
    "In our case it's one equation because:\n",
    "\n",
    "- From previous sections we found out that $A$ is full rank, thus $A^TA$ is invertible.\n",
    "- $W$ is positive definite\n",
    "- From both points above, we can conclude that $A^TWA$ is invertible, thus one equation\n",
    "- $A^TWA$ is also squared, dimensions check: $NxM * MxM * MxN = NxM * MxN = NxN$\n",
    "\n",
    "We can now use np.linalg.solve() which is more efficient than np.linalg.lstsq() because it not use the SVD form \n",
    "Note: linalg.solve() require that the matrix passed - in our case it's $A^TWA$ - is squared and non-singular which we already confirmed.\n",
    "\n",
    "We will go with a trial-and-error approach here to meet the soft requirement (\"let's say < 1e -3\") (we could use binary search but this is overkill for our purpose and the soft requirement) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5853d4b7-dc57-468d-b880-ac995b0f6d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 10, |r_2| = [0.0205]\n",
      "Weight: 100, |r_2| = [0.0021]\n",
      "Weight: 1000, |r_2| = [0.0002]\n",
      "Requirement met\n",
      "The solution x after requirement met:\n",
      "[[1.7059]\n",
      " [0.6764]\n",
      " [0.6471]]\n",
      "The residual vector r after requirement met:\n",
      "[[-6.1763e-01]\n",
      " [ 2.0588e-04]\n",
      " [-1.1369e-13]\n",
      " [ 1.0294e+00]]\n"
     ]
    }
   ],
   "source": [
    "for weight in [10, 100, 1000, 5000]:\n",
    "    W = np.eye(4)\n",
    "    W[1, 1] = weight  # second row, second column - the second entry which affect the second equation\n",
    "\n",
    "    # left and right side of the normal equation\n",
    "    left_side = A.T @ W @ A       # (A^T W A)\n",
    "    right_side = A.T @ W @ b       # (A^T W b)\n",
    "    x_w = np.linalg.solve(left_side, right_side)\n",
    "\n",
    "    # Compute residual vector using the original A and b\n",
    "    r = A @ x_w - b\n",
    "    print(f\"Weight: {weight}, |r_2| = {abs(r[1])}\")\n",
    "\n",
    "    if abs(r[1]) < 1e-3:\n",
    "        print(\"Requirement met\")\n",
    "        print(f\"The solution x after requirement met:\\n{x_w}\")\n",
    "        print(f\"The residual vector r after requirement met:\\n{r}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efe972-e4c6-42eb-b211-f6677493439d",
   "metadata": {},
   "source": [
    "We can see that while we improved residual 2, other residuals got \"hurt\", such as residual 4, but that's makes sense."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5288bbb-0e7c-4941-b4e3-794cabd3ee7e",
   "metadata": {},
   "source": [
    "### Section (e)\n",
    "\n",
    "We now need to solve regularized LS, which $λ = 0.5$ and $C = I$ (Tikhonov regularization term) (other name - Ridge regression)\n",
    "\n",
    "We want to use np.linalg.solve() so we will speak on this form of the normal equation - only one, we talked about it in class and it is written in the notes:\n",
    "$$\n",
    "(A^TA + λI)x = A^Tb \n",
    "$$\n",
    "(because linalg.solve() takes left side and right side of a general Ax=b system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b250264-b5ef-4294-a86b-bea7f856f232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized LS solution x:\n",
      "[[1.3852]\n",
      " [0.535 ]\n",
      " [0.8896]]\n"
     ]
    }
   ],
   "source": [
    "λ = 0.5\n",
    "n = A.shape[1]\n",
    "C = np.eye(n)  # Identity matrix\n",
    "\n",
    "left_side = A.T @ A + λ * (C.T @ C)  # Equivalent to A^T A + λ * I\n",
    "right_side = A.T @ b\n",
    "\n",
    "x_reg = np.linalg.solve(left_side, right_side)\n",
    "\n",
    "print(f\"Regularized LS solution x:\\n{x_reg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
